{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 9.3\n",
    "\n",
    "Brandon Sams\n",
    "\n",
    "12May2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Neural Network Classifier with Scikit\n",
    "\n",
    "Using the multi-label classifier dataset from earlier exercises ([categorized-comments.jsonl](https://content.bellevue.edu/cst/dsc/550/id/data-files-and-sample-code/week-9.zip) in the reddit folder), fit a neural network classifier using scikit-learn. Use the code found in chapter 12 of the Applied Text Analysis with Python book as a guideline. Report the accuracy, precision, recall, F1-score, and confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, json, re, pickle\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, auc, precision_recall_fscore_support\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file):\n",
    "    \"\"\"\n",
    "    Take a json file location and\n",
    "    read the file into a pandas data frame\n",
    "    Args: full path to file\n",
    "    Returns: pandas dataframe with data from file\n",
    "    \"\"\"\n",
    "    \n",
    "    data = []\n",
    "\n",
    "    with open(file) as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "        \n",
    "    # convert to data frame\n",
    "    \n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 606476 entries, 0 to 606475\nData columns (total 2 columns):\n #   Column  Non-Null Count   Dtype \n---  ------  --------------   ----- \n 0   cat     606476 non-null  object\n 1   txt     606476 non-null  object\ndtypes: object(2)\nmemory usage: 9.3+ MB\nSize:  606476 \n Shape:  None \n Categories:  ['sports' 'science_and_technology' 'video_games']\n"
    }
   ],
   "source": [
    "# read controversy data\n",
    "\n",
    "con_df = read_data('categorized-comments.jsonl')\n",
    "\n",
    "# check size, structure and categories\n",
    "\n",
    "print('Size: ', len(con_df), '\\n',\n",
    "      'Shape: ', con_df.info(), '\\n',\n",
    "      'Categories: ', con_df.cat.unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Remove punctuations and special characters, makes lower case\n",
    "    Args: text \n",
    "    Output: text\n",
    "    \"\"\"\n",
    "    \n",
    "    text=text.lower()\n",
    "    text=re.sub('</?.*?>',' <>', text)\n",
    "    text=re.sub('\\\\d|\\\\W+|_',' ',text)\n",
    "    text=re.sub('[^a-zA-Z]',\" \", text)\n",
    "    \n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create stop words list\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def tokenize_remove_stopwords(txt):\n",
    "    token_txt = word_tokenize(txt)\n",
    "    no_stop_txt = [word for word in token_txt if word not in stop_words]\n",
    "    no_stop_string = ' '.join(no_stop_txt)\n",
    "    return(no_stop_string)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                      cat                                                txt  \\\n0  science_and_technology  which is good for privacy they store it for up...   \n1  science_and_technology  agreed it s not unheard of but it is still rar...   \n2  science_and_technology  the huawei honor   is pretty good it has bands...   \n3  science_and_technology   gt we don t know this yet they struggle https...   \n4  science_and_technology                                               omfg   \n\n   cat_codes  \n0          0  \n1          0  \n2          0  \n3          0  \n4          0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cat</th>\n      <th>txt</th>\n      <th>cat_codes</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>science_and_technology</td>\n      <td>which is good for privacy they store it for up...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>science_and_technology</td>\n      <td>agreed it s not unheard of but it is still rar...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>science_and_technology</td>\n      <td>the huawei honor   is pretty good it has bands...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>science_and_technology</td>\n      <td>gt we don t know this yet they struggle https...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>science_and_technology</td>\n      <td>omfg</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "# since the size is humongus, I will take sample of the 2 categories. \n",
    "# by trial, sample of 50000 from each category can be easily handled by my machine\n",
    "\n",
    "size = 50000    # sample size\n",
    "replace = True  # with replacement\n",
    "fn = lambda obj: obj.loc[np.random.choice(obj.index, size, replace),:]\n",
    "\n",
    "categories = con_df.groupby('cat', as_index=False).apply(fn)\n",
    "\n",
    "# free up memory\n",
    "\n",
    "del con_df\n",
    "\n",
    "categories['txt'] = categories['txt'].apply(lambda x:clean_text(x))\n",
    "#categories['txt'] = categories['txt'].apply(tokenize_remove_stopwords)\n",
    "categories.reset_index(drop=True, inplace=True)\n",
    "\n",
    "categories.cat = pd.Categorical(categories.cat)\n",
    "categories['cat_codes'] = categories.cat.cat.codes\n",
    "\n",
    "categories.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                           cat  \\\n0       science_and_technology   \n1       science_and_technology   \n2       science_and_technology   \n3       science_and_technology   \n4       science_and_technology   \n...                        ...   \n149995             video_games   \n149996             video_games   \n149997             video_games   \n149998             video_games   \n149999             video_games   \n\n                                                      txt  cat_codes  \\\n0       which is good for privacy they store it for up...          0   \n1       agreed it s not unheard of but it is still rar...          0   \n2       the huawei honor   is pretty good it has bands...          0   \n3        gt we don t know this yet they struggle https...          0   \n4                                                    omfg          0   \n...                                                   ...        ...   \n149995  good to know i ll keep that in mind for the ne...          2   \n149996                                        ds remakes           2   \n149997                                  invoker or slark           2   \n149998     it s mostly the hype generated by terry crews           2   \n149999  you have problems kid i love how you re acting...          2   \n\n        science_and_technology  sports  video_games  \n0                            1       0            0  \n1                            1       0            0  \n2                            1       0            0  \n3                            1       0            0  \n4                            1       0            0  \n...                        ...     ...          ...  \n149995                       0       0            1  \n149996                       0       0            1  \n149997                       0       0            1  \n149998                       0       0            1  \n149999                       0       0            1  \n\n[150000 rows x 6 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cat</th>\n      <th>txt</th>\n      <th>cat_codes</th>\n      <th>science_and_technology</th>\n      <th>sports</th>\n      <th>video_games</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>science_and_technology</td>\n      <td>which is good for privacy they store it for up...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>science_and_technology</td>\n      <td>agreed it s not unheard of but it is still rar...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>science_and_technology</td>\n      <td>the huawei honor   is pretty good it has bands...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>science_and_technology</td>\n      <td>gt we don t know this yet they struggle https...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>science_and_technology</td>\n      <td>omfg</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>149995</th>\n      <td>video_games</td>\n      <td>good to know i ll keep that in mind for the ne...</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>149996</th>\n      <td>video_games</td>\n      <td>ds remakes</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>149997</th>\n      <td>video_games</td>\n      <td>invoker or slark</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>149998</th>\n      <td>video_games</td>\n      <td>it s mostly the hype generated by terry crews</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>149999</th>\n      <td>video_games</td>\n      <td>you have problems kid i love how you re acting...</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>150000 rows Ã— 6 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "pd.concat([categories, pd.get_dummies(categories.cat)],axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidfconverter = TfidfVectorizer(max_features=1500, stop_words=stop_words)\n",
    "X = tfidfconverter.fit_transform(categories.txt).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, categories.cat_codes, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Iteration 1, loss = 0.71525282\nIteration 2, loss = 0.60867110\nIteration 3, loss = 0.56798250\nIteration 4, loss = 0.51434957\nIteration 5, loss = 0.46800649\nIteration 6, loss = 0.42675279\nIteration 7, loss = 0.39082716\nIteration 8, loss = 0.35899659\nIteration 9, loss = 0.33014425\nIteration 10, loss = 0.30797898\nIteration 11, loss = 0.28739397\nIteration 12, loss = 0.27218620\nIteration 13, loss = 0.26108569\nIteration 14, loss = 0.25401585\nIteration 15, loss = 0.24630976\nIteration 16, loss = 0.24024005\nIteration 17, loss = 0.23679391\nIteration 18, loss = 0.23302999\nIteration 19, loss = 0.23129372\nIteration 20, loss = 0.22802770\n"
    }
   ],
   "source": [
    "classifier = MLPClassifier(hidden_layer_sizes=(150,30,15,15),random_state=1, max_iter=20, verbose=True, activation = 'relu', solver='adam')\n",
    "classifier.fit(X_train, y_train)    \n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "precision    recall  f1-score   support\n\n           0       0.84      0.86      0.85      9887\n           1       0.68      0.77      0.72      9987\n           2       0.74      0.62      0.68     10126\n\n    accuracy                           0.75     30000\n   macro avg       0.75      0.75      0.75     30000\nweighted avg       0.75      0.75      0.75     30000\n\n"
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[8540  787  560]\n [ 620 7686 1681]\n [ 953 2859 6314]]\n"
    }
   ],
   "source": [
    "print(confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.7513333333333333\n"
    }
   ],
   "source": [
    "print(accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Neural Network Classifier with Keras\n",
    "\n",
    "Using the multi-label classifier dataset from earlier exercises (categorized-comments.jsonl in the reddit folder), fit a neural network classifier using Keras. Use the code found in chapter 12 of the Applied Text Analysis with Python book as a guideline. Report the accuracy, precision, recall, F1-score, and confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Using TensorFlow backend.\n"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "import keras\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X_train.shape[1]  # Number of features\n",
    "\n",
    "model = Sequential()\n",
    "model.add(keras.layers.Dense(150, input_dim=input_dim, activation='relu'))\n",
    "model.add(keras.layers.Dense(30, activation='sigmoid'))\n",
    "model.add(keras.layers.Dense(15, activation='sigmoid'))\n",
    "model.add(keras.layers.Dense(15, activation='sigmoid'))\n",
    "model.add(keras.layers.Dense(3, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_1 (Dense)              (None, 150)               225150    \n_________________________________________________________________\ndense_2 (Dense)              (None, 30)                4530      \n_________________________________________________________________\ndense_3 (Dense)              (None, 15)                465       \n_________________________________________________________________\ndense_4 (Dense)              (None, 15)                240       \n_________________________________________________________________\ndense_5 (Dense)              (None, 3)                 48        \n=================================================================\nTotal params: 230,433\nTrainable params: 230,433\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_keras = keras.utils.to_categorical(y_train)\n",
    "y_test_keras = keras.utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 120000 samples, validate on 30000 samples\nEpoch 1/20\n120000/120000 [==============================] - 523s 4ms/step - loss: 0.4182 - accuracy: 0.7983 - val_loss: 0.3763 - val_accuracy: 0.8158\nEpoch 2/20\n120000/120000 [==============================] - 391s 3ms/step - loss: 0.3631 - accuracy: 0.8213 - val_loss: 0.3647 - val_accuracy: 0.8196\nEpoch 3/20\n120000/120000 [==============================] - 438s 4ms/step - loss: 0.3393 - accuracy: 0.8326 - val_loss: 0.3538 - val_accuracy: 0.8260\nEpoch 4/20\n120000/120000 [==============================] - 402s 3ms/step - loss: 0.3088 - accuracy: 0.8484 - val_loss: 0.3477 - val_accuracy: 0.8324\nEpoch 5/20\n120000/120000 [==============================] - 504s 4ms/step - loss: 0.2773 - accuracy: 0.8647 - val_loss: 0.3525 - val_accuracy: 0.8344\nEpoch 6/20\n120000/120000 [==============================] - 576s 5ms/step - loss: 0.2481 - accuracy: 0.8793 - val_loss: 0.3553 - val_accuracy: 0.8393\nEpoch 7/20\n120000/120000 [==============================] - 516s 4ms/step - loss: 0.2244 - accuracy: 0.8911 - val_loss: 0.3764 - val_accuracy: 0.8404\nEpoch 8/20\n120000/120000 [==============================] - 505s 4ms/step - loss: 0.2043 - accuracy: 0.9003 - val_loss: 0.3899 - val_accuracy: 0.8408\nEpoch 9/20\n120000/120000 [==============================] - 500s 4ms/step - loss: 0.1882 - accuracy: 0.9078 - val_loss: 0.4191 - val_accuracy: 0.8429\nEpoch 10/20\n120000/120000 [==============================] - 472s 4ms/step - loss: 0.1763 - accuracy: 0.9138 - val_loss: 0.4369 - val_accuracy: 0.8430\nEpoch 11/20\n120000/120000 [==============================] - 538s 4ms/step - loss: 0.1667 - accuracy: 0.9179 - val_loss: 0.4674 - val_accuracy: 0.8423\nEpoch 12/20\n120000/120000 [==============================] - 688s 6ms/step - loss: 0.1596 - accuracy: 0.9209 - val_loss: 0.4768 - val_accuracy: 0.8438\nEpoch 13/20\n120000/120000 [==============================] - 519s 4ms/step - loss: 0.1538 - accuracy: 0.9238 - val_loss: 0.5165 - val_accuracy: 0.8416\nEpoch 14/20\n120000/120000 [==============================] - 483s 4ms/step - loss: 0.1491 - accuracy: 0.9255 - val_loss: 0.5160 - val_accuracy: 0.8424\nEpoch 15/20\n120000/120000 [==============================] - 385s 3ms/step - loss: 0.1446 - accuracy: 0.9275 - val_loss: 0.5419 - val_accuracy: 0.8408\nEpoch 16/20\n120000/120000 [==============================] - 382s 3ms/step - loss: 0.1413 - accuracy: 0.9292 - val_loss: 0.5538 - val_accuracy: 0.8431\nEpoch 17/20\n120000/120000 [==============================] - 379s 3ms/step - loss: 0.1392 - accuracy: 0.9301 - val_loss: 0.5546 - val_accuracy: 0.8406\nEpoch 18/20\n120000/120000 [==============================] - 379s 3ms/step - loss: 0.1366 - accuracy: 0.9313 - val_loss: 0.5761 - val_accuracy: 0.8411\nEpoch 19/20\n120000/120000 [==============================] - 414s 3ms/step - loss: 0.1346 - accuracy: 0.9321 - val_loss: 0.5876 - val_accuracy: 0.8420\nEpoch 20/20\n120000/120000 [==============================] - 419s 3ms/step - loss: 0.1327 - accuracy: 0.9329 - val_loss: 0.5822 - val_accuracy: 0.8429\n"
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train_keras,\n",
    "                    epochs=20,\n",
    "                    verbose=True,\n",
    "                    validation_data=(X_test, y_test_keras),\n",
    "                    batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Training Accuracy: 0.9361\nTesting Accuracy:  0.8429\n"
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_train, y_train_keras, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(X_test, y_test_keras, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_keras_results = tf.argmax(y_test_keras,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "30000/30000 [==============================] - 4s 122us/step\n"
    }
   ],
   "source": [
    "y_pred_keras_results = tf.argmax(model.predict(X_test, verbose = 1),axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "precision    recall  f1-score   support\n\n           0       0.84      0.87      0.85      9887\n           1       0.68      0.76      0.72      9987\n           2       0.74      0.63      0.68     10126\n\n    accuracy                           0.75     30000\n   macro avg       0.75      0.75      0.75     30000\nweighted avg       0.75      0.75      0.75     30000\n\n"
    }
   ],
   "source": [
    "print(classification_report(y_test_keras_results, y_pred_keras_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[8565  748  574]\n [ 658 7638 1691]\n [ 979 2795 6352]]\n"
    }
   ],
   "source": [
    "print(confusion_matrix(y_test_keras_results, y_pred_keras_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.7518333333333334\n"
    }
   ],
   "source": [
    "print(accuracy_score(y_test_keras_results, y_pred_keras_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Classifying Images\n",
    "\n",
    "In chapter 20 of the Machine Learning with Python Cookbook, implement the code found in section 20.15 classify MSINT images using a convolutional neural network. Report the accuracy of your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as numpy\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set that the color channel value will be first\n",
    "K.set_image_data_format(\"channels_first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set image information\n",
    "channels = 1\n",
    "height = 28\n",
    "width = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and target from MNIST data\n",
    "(data_train, target_train), (data_test, target_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape training image data into features\n",
    "data_train = data_train.reshape(data_train.shape[0],channels, height, width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape test image data into features\n",
    "data_test = data_test.reshape(data_test.shape[0], channels, height, width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rescale pixel intensity to between 0 and 1\n",
    "features_train = data_train / 255\n",
    "features_test = data_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode target\n",
    "target_train = np_utils.to_categorical(target_train)\n",
    "target_test = np_utils.to_categorical(target_test)\n",
    "number_of_classes = target_test.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start neural network\n",
    "network = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add convolutional layer with 64 filters, a 5x5 window, and ReLU activation function\n",
    "network.add(Conv2D(filters=64,\n",
    "                   kernel_size=(5, 5),\n",
    "                   input_shape=(channels, width, height),\n",
    "                   activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add max pooling layer with a 2x2 window\n",
    "network.add(MaxPooling2D(pool_size=(2, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add dropout layer\n",
    "network.add(Dropout(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add layer to flatten input\n",
    "network.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add fully connected layer of 128 units with a ReLU activation function\n",
    "network.add(Dense(128, activation=\"relu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add dropout layer\n",
    "network.add(Dropout(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add fully connected layer with a softmax activation function\n",
    "network.add(Dense(number_of_classes, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile neural network\n",
    "network.compile(loss=\"categorical_crossentropy\", # Cross-entropy\n",
    "                optimizer=\"rmsprop\", # Root Mean Square Propagation\n",
    "                metrics=[\"accuracy\"]) # Accuracy performance metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 60000 samples, validate on 10000 samples\nEpoch 1/20\n60000/60000 [==============================] - 293s 5ms/step - loss: 0.5912 - accuracy: 0.8175 - val_loss: 0.1895 - val_accuracy: 0.9445\nEpoch 2/20\n60000/60000 [==============================] - 280s 5ms/step - loss: 0.1926 - accuracy: 0.9431 - val_loss: 0.0917 - val_accuracy: 0.9722\nEpoch 3/20\n60000/60000 [==============================] - 277s 5ms/step - loss: 0.1254 - accuracy: 0.9632 - val_loss: 0.0627 - val_accuracy: 0.9800\nEpoch 4/20\n60000/60000 [==============================] - 277s 5ms/step - loss: 0.0983 - accuracy: 0.9707 - val_loss: 0.0500 - val_accuracy: 0.9843\nEpoch 5/20\n60000/60000 [==============================] - 278s 5ms/step - loss: 0.0812 - accuracy: 0.9758 - val_loss: 0.0419 - val_accuracy: 0.9858\nEpoch 6/20\n60000/60000 [==============================] - 283s 5ms/step - loss: 0.0698 - accuracy: 0.9797 - val_loss: 0.0383 - val_accuracy: 0.9874\nEpoch 7/20\n60000/60000 [==============================] - 278s 5ms/step - loss: 0.0617 - accuracy: 0.9811 - val_loss: 0.0364 - val_accuracy: 0.9882\nEpoch 8/20\n60000/60000 [==============================] - 277s 5ms/step - loss: 0.0578 - accuracy: 0.9829 - val_loss: 0.0333 - val_accuracy: 0.9886\nEpoch 9/20\n60000/60000 [==============================] - 277s 5ms/step - loss: 0.0537 - accuracy: 0.9834 - val_loss: 0.0333 - val_accuracy: 0.9886\nEpoch 10/20\n60000/60000 [==============================] - 277s 5ms/step - loss: 0.0498 - accuracy: 0.9844 - val_loss: 0.0311 - val_accuracy: 0.9894\nEpoch 11/20\n60000/60000 [==============================] - 277s 5ms/step - loss: 0.0462 - accuracy: 0.9858 - val_loss: 0.0280 - val_accuracy: 0.9904\nEpoch 12/20\n60000/60000 [==============================] - 277s 5ms/step - loss: 0.0437 - accuracy: 0.9864 - val_loss: 0.0290 - val_accuracy: 0.9903\nEpoch 13/20\n60000/60000 [==============================] - 277s 5ms/step - loss: 0.0392 - accuracy: 0.9880 - val_loss: 0.0282 - val_accuracy: 0.9896\nEpoch 14/20\n60000/60000 [==============================] - 287s 5ms/step - loss: 0.0393 - accuracy: 0.9875 - val_loss: 0.0271 - val_accuracy: 0.9908\nEpoch 15/20\n60000/60000 [==============================] - 277s 5ms/step - loss: 0.0379 - accuracy: 0.9884 - val_loss: 0.0278 - val_accuracy: 0.9908\nEpoch 16/20\n60000/60000 [==============================] - 277s 5ms/step - loss: 0.0341 - accuracy: 0.9893 - val_loss: 0.0269 - val_accuracy: 0.9910\nEpoch 17/20\n60000/60000 [==============================] - 277s 5ms/step - loss: 0.0337 - accuracy: 0.9893 - val_loss: 0.0281 - val_accuracy: 0.9908\nEpoch 18/20\n60000/60000 [==============================] - 278s 5ms/step - loss: 0.0327 - accuracy: 0.9898 - val_loss: 0.0252 - val_accuracy: 0.9917\nEpoch 19/20\n60000/60000 [==============================] - 277s 5ms/step - loss: 0.0313 - accuracy: 0.9904 - val_loss: 0.0273 - val_accuracy: 0.9910\nEpoch 20/20\n60000/60000 [==============================] - 276s 5ms/step - loss: 0.0300 - accuracy: 0.9907 - val_loss: 0.0266 - val_accuracy: 0.9906\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<keras.callbacks.callbacks.History at 0x1a28647c90>"
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "source": [
    "# Train neural network\n",
    "network.fit(features_train, # Features\n",
    "            target_train, # Target\n",
    "            epochs=20, # Number of epochs\n",
    "            verbose=1, # Print description after each epoch\n",
    "            batch_size=1000, # Number of observations per batch\n",
    "            validation_data=(features_test, target_test)) # Data for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "10000/10000 [==============================] - 25s 3ms/step\nThe accuracy of the model, when compared against the testing data, is 99.0600%\n"
    }
   ],
   "source": [
    "val_loss, val_accuracy = network.evaluate(features_test, target_test)\n",
    "\n",
    "print(\"The accuracy of the model, when compared against the testing data, is {:.4f}%\".format(val_accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37664bitdsc550condad893da3c7aad4165be1b12c1c9b9e87e",
   "display_name": "Python 3.7.6 64-bit ('DSC550': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}